# -*- coding: utf-8 -*-
"""camaroes_sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U2OJ-QEuMRUKy2f5vlHojOoYfrQOhhii

# Normalização
"""


import pandas as pd
import string
import re
import unidecode
import nltk
from nltk.tokenize import word_tokenize, RegexpTokenizer

nltk.download('stopwords')

df = pd.read_csv("C:\\Users\\Gusto\\Desktop\\projects\\camaroes\\camaroes.csv")

df["Comentarios"].describe()

df["Comentarios"].unique()

df["Estrelas"].describe()

def converter_minusculo(text):
    return text.lower()

def remove_espaco_branco(text):
    return text.strip()

def remove_pontuacao(text):
    punct_str = string.punctuation
    punct_str = punct_str.replace("'", "")
    translator = str.maketrans("", "", punct_str)
    return text.translate(translator)

def remove_emoji(text):
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"
        u"\U0001F300-\U0001F5FF"
        u"\U0001F680-\U0001F6FF"
        u"\U0001F1E0-\U0001F1FF"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r"", text)

def remove_http(text):
    http = r"https?:\/\/(?:www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b(?:[-a-zA-Z0-9()@:%_\+.~#?&\/=]*)"
    pattern = re.compile(http, re.IGNORECASE)
    return pattern.sub("", text)

regexp = RegexpTokenizer(r"\b\w+\b")
linguas = ['portuguese', 'english', 'spanish', 'french']
stops = []
for lingua in linguas:
    stops += nltk.corpus.stopwords.words(lingua)
def remove_stopwords(text):
    return " ".join([word for word in regexp.tokenize(text) if word not in stops])

def text_normalizer(text):
    text = unidecode.unidecode(text)
    text = re.sub('\n', '', text)
    text = remove_http(text)
    text = remove_emoji(text)
    text = remove_pontuacao(text)
    text = remove_espaco_branco(text)
    text = converter_minusculo(text)
    text = remove_stopwords(text)
    return text


data = pd.DataFrame()
data["titulo_norm"] = df["Título"].apply(text_normalizer)
data["Coments_norm"] = df["Comentarios"].apply(text_normalizer)
df["Coments_norm"] = data["Coments_norm"].to_list()

data["Estrelas"] = df["Estrelas"]

"""# Análise de sentimentos"""


nltk.download('vader_lexicon')

from nltk.sentiment import SentimentIntensityAnalyzer
#from translate import Translator
from googletrans import Translator
import tqdm
import time
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

sia = SentimentIntensityAnalyzer()

#translator = Translator(from_lang="pt-br", to_lang="en")
translator = Translator()

traducao = translator.translate(data.iloc[0]["Coments_norm"])

x = sia.polarity_scores(traducao.text)


def traduzir(text):
    #translator = Translator(from_lang="pt-br", to_lang="en")
    #traducao = translator.translate(str(text))
    translator = Translator()
    traducao = translator.translate(str(text), dest='en').text
    return traducao

t = traduzir('olá')

df2 = data.head(5)

pol = []
cont = 0

inicio = time.time()
for i, row in data.iterrows():
    text = row['Coments_norm']
    text_t = traduzir(text)
    text_t_norm = text_normalizer(text_t)
    print(cont)
    cont+=1
    time.sleep(2)
    pol.append(sia.polarity_scores(text_t_norm))
    data.loc[i, 'Coments_norm'] = text_t_norm
fim = time.time()

print(f'Tempo total: {fim-inicio} segundos')
print(f'Tempo total: {(fim-inicio)/60} minutos')

polarity = []
for score in pol:
    compound = score['compound']
    neg = score['neg']
    neu = score['neu']
    pos = score['pos']

    if compound < 0:
        polarity.append("negativo")
    elif neg < 0.1 and neg > 0 and pos < 0.3 and neu > 0.6:
        polarity.append("neutro")
    elif neg >= 0.1:
        polarity.append("negativo")
    elif neu >= 0.7 and pos <= 0.2:
        polarity.append("neutro")
    else:
        polarity.append("positivo")

polarity

data['polaridade'] = polarity

def classificar_estrelas(estrelas):
    if estrelas >= 4:
        return "positivo"
    elif estrelas <= 2:
        return "negativo"
    else:
        return "neutro"

data['classificacao_estrelas'] = data['Estrelas'].apply(classificar_estrelas)

print(data[['Estrelas','classificacao_estrelas','polaridade']])

cm = confusion_matrix(data['classificacao_estrelas'], data['polaridade'])

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Negativo', 'Neutro', 'Positivo'],
            yticklabels=['Negativo', 'Neutro', 'Positivo'])
plt.xlabel('Polaridade Predita')
plt.ylabel('Classificação por Estrelas')
plt.title('Matriz de Confusão')
plt.show()

import csv

with open('C:\\Users\\Gusto\\Desktop\\projects\\camaroes\\camaroes_semtiment.csv', 'w', newline='', encoding='utf-8') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['Coments_norm', 'Polaridade', 'Estrelas', 'Classificacao_estrelas'])

    for index, row in data.iterrows():
        writer.writerow([row['Coments_norm'], row['polaridade'], row['Estrelas'], row['classificacao_estrelas']])
